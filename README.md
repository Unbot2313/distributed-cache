# üöÄ Distributed Cache

> A high-performance, horizontally-scalable distributed cache system built with Go, designed to compete with Redis Cluster while focusing on learning distributed systems concepts.

![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=for-the-badge&logo=go)
![Build Status](https://img.shields.io/badge/Build-Passing-success?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-blue?style=for-the-badge)

## üéØ Project Vision

**Distributed Cache** is a learning-focused project that implements a production-ready distributed caching system from scratch. The goal is to understand and implement core distributed systems concepts while building something that can compete with industry standards like Redis Cluster.

### Why This Project?

- üìö **Learning Distributed Systems**: Hands-on experience with consensus algorithms, fault tolerance, and distributed computing
- ‚ö° **Performance-Focused**: Built with speed as a primary concern, leveraging Go's concurrency model
- üîÑ **Modern Architecture**: Uses cutting-edge tools like DragonFly as the storage backend
- üõ†Ô∏è **Production-Ready**: Designed with real-world scalability and reliability requirements

---

## ‚ú® Key Features

### üî• Core Functionality
- **Consistent Hashing**: Efficient key distribution with virtual nodes for optimal load balancing
- **Horizontal Scaling**: Add/remove nodes seamlessly without service interruption  
- **High Performance**: Sub-millisecond response times with xxHash3 for lightning-fast hashing
- **Fault Tolerance**: Automatic failover and data replication across nodes

### üåê Distributed Systems Concepts
- **Gossip Protocol**: Efficient node discovery and cluster membership management
- **Quorum Consensus**: Configurable consistency levels (R + W > N) for different use cases
- **Automatic Rebalancing**: Smart data migration when cluster topology changes
- **Health Monitoring**: Real-time cluster health and performance metrics

### üèóÔ∏è Architecture
- **Pluggable Storage**: Support for DragonFly, Redis, or any cache backend
- **Smart Client**: Client-side routing to minimize proxy overhead
- **Kubernetes Ready**: Native K8s support with StatefulSets and service discovery
- **Observability**: Built-in Prometheus metrics and distributed tracing

---

## üèõÔ∏è Architecture Overview

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Load Balancer ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Proxy/Router   ‚îÇ
                    ‚îÇ (Consistent     ‚îÇ
                    ‚îÇ  Hashing)       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                    ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Node 1  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ Node 2  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Node 3  ‚îÇ
    ‚îÇDragonFly‚îÇ  Gossip ‚îÇDragonFly‚îÇ  Gossip ‚îÇDragonFly‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         Replication
```

### Hash Ring Distribution
```
                    Hash Space (0 - 2¬≥¬≤)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  [N1] ‚îÄ‚îÄ‚îÄ‚îÄ [N3] ‚îÄ‚îÄ [N2] ‚îÄ‚îÄ‚îÄ‚îÄ [N1] ‚îÄ‚îÄ [N3] ‚îÄ‚îÄ   ‚îÇ
    ‚îÇ    ‚ñ≤         ‚ñ≤       ‚ñ≤         ‚ñ≤       ‚ñ≤        ‚îÇ
    ‚îÇ Virtual   Virtual Virtual   Virtual Virtual     ‚îÇ
    ‚îÇ  Nodes     Nodes   Nodes     Nodes   Nodes     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### Prerequisites
- **Go 1.21+**
- **Docker & Docker Compose**
- **DragonFly** or **Redis** (for storage backend)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/distributed-cache.git
cd distributed-cache

# Install dependencies
go mod download

# Run tests
go test ./...

# Start a 5-node cluster
docker-compose up
```

### Basic Usage

```go
package main

import (
    "github.com/yourusername/distributed-cache/pkg/client"
    "github.com/yourusername/distributed-cache/pkg/hash"
    "github.com/yourusername/distributed-cache/pkg/ring"
)

func main() {
    // Create a hash ring
    hasher := hash.NewXXH3Hasher()
    hashRing := ring.NewRing(hasher, 150) // 150 virtual nodes per physical node
    
    // Add cache nodes
    hashRing.AddNode("cache-node-1:8080")
    hashRing.AddNode("cache-node-2:8080") 
    hashRing.AddNode("cache-node-3:8080")
    
    // Route keys to appropriate nodes
    node := hashRing.GetNode("user:12345")
    fmt.Printf("Key 'user:12345' routes to: %s\n", node)
    
    // Create cache client
    client := client.New(hashRing)
    
    // Cache operations
    client.Set("user:12345", userData)
    data, err := client.Get("user:12345")
    client.Delete("user:12345")
}
```

---

## üìÇ Project Structure

```
distributed-cache/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îú‚îÄ‚îÄ cache-node/          # Cache server executable
‚îÇ   ‚îú‚îÄ‚îÄ cache-proxy/         # Proxy/router service  
‚îÇ   ‚îî‚îÄ‚îÄ benchmark/           # Performance testing tools
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îú‚îÄ‚îÄ hash/                # Hash functions (xxHash3)
‚îÇ   ‚îú‚îÄ‚îÄ ring/                # Consistent hashing implementation
‚îÇ   ‚îú‚îÄ‚îÄ gossip/              # Gossip protocol for node discovery
‚îÇ   ‚îú‚îÄ‚îÄ storage/             # Storage backends (DragonFly, Redis)
‚îÇ   ‚îú‚îÄ‚îÄ quorum/              # Quorum consensus implementation  
‚îÇ   ‚îú‚îÄ‚îÄ client/              # Smart client library
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/          # Metrics and observability
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml   # 5-node development cluster
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile           # Container images
‚îú‚îÄ‚îÄ k8s/                     # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ statefulset.yml      # Cache nodes
‚îÇ   ‚îú‚îÄ‚îÄ service.yml          # Service discovery
‚îÇ   ‚îî‚îÄ‚îÄ configmap.yml        # Configuration
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml       # Metrics collection
‚îÇ   ‚îú‚îÄ‚îÄ grafana/             # Dashboards
‚îÇ   ‚îî‚îÄ‚îÄ alerts.yml           # Alerting rules
‚îî‚îÄ‚îÄ benchmarks/              # Performance tests and comparisons
    ‚îú‚îÄ‚îÄ redis-comparison/    # Benchmark vs Redis Cluster  
    ‚îî‚îÄ‚îÄ load-tests/          # Stress testing scenarios
```

---

## üéØ Implementation Roadmap

### ‚úÖ Phase 1: Core Foundation
- [x] Consistent Hashing with Virtual Nodes
- [x] xxHash3 Implementation  
- [x] Basic Ring Operations (Add/Remove/Get)
- [x] Unit Tests & Benchmarks

### üöß Phase 2: Storage Integration (In Progress)
- [ ] DragonFly Backend Integration
- [ ] Redis Backend Support
- [ ] Storage Interface Abstraction
- [ ] Connection Pooling & Management

### üìã Phase 3: Distributed Coordination  
- [ ] Gossip Protocol Implementation
- [ ] Node Discovery & Health Monitoring
- [ ] Automatic Failure Detection
- [ ] Cluster Membership Management

### üìã Phase 4: Consensus & Consistency
- [ ] Quorum-based Operations (R + W > N)
- [ ] Configurable Consistency Levels
- [ ] Read Repair Mechanisms
- [ ] Anti-Entropy Protocols

### üìã Phase 5: Production Features
- [ ] Data Migration & Rebalancing
- [ ] Prometheus Metrics Export
- [ ] Kubernetes Native Deployment
- [ ] Performance Optimization

### üìã Phase 6: Advanced Features
- [ ] Multi-Datacenter Replication
- [ ] Compression & Serialization Options
- [ ] TTL & Expiration Policies  
- [ ] Admin Dashboard & Monitoring

---

## ‚ö° Performance Goals

### Target Benchmarks
| Operation | Target Latency | Target Throughput |
|-----------|---------------|-------------------|
| GET       | < 1ms P99     | > 100K ops/sec    |
| SET       | < 2ms P99     | > 80K ops/sec     |
| DELETE    | < 1ms P99     | > 90K ops/sec     |

### vs Redis Cluster Comparison
- **Latency**: Aim for comparable P95/P99 latencies
- **Throughput**: Target 80%+ of Redis Cluster performance  
- **Memory Efficiency**: Competitive memory usage per key
- **Network Overhead**: Minimize inter-node communication

---

## üß™ Testing & Development

### Running Tests
```bash
# Unit tests
go test ./pkg/...

# Integration tests
go test ./integration/...

# Benchmarks
go test -bench=. ./pkg/hash/
go test -bench=. ./pkg/ring/

# Race condition detection
go test -race ./...
```

### Development Setup
```bash
# Start development cluster
make dev-cluster

# Run with hot reload
make dev-server

# Generate test data
make test-data

# Performance profiling
make profile
```

---

## üìä Monitoring & Observability

### Metrics Exposed
- **Cache Hit Ratio**: Percentage of successful cache retrievals
- **Request Latency**: P50, P95, P99 response times by operation
- **Node Health**: CPU, Memory, Network usage per node
- **Ring Distribution**: Key distribution balance across nodes
- **Gossip Activity**: Membership changes and failure detection

### Grafana Dashboards
- **Cluster Overview**: High-level system health and performance
- **Node Details**: Per-node metrics and resource usage
- **Cache Performance**: Hit ratios, latency distributions
- **Network Activity**: Inter-node communication patterns

---

## ü§ù Contributing

This is primarily a learning project, but contributions are welcome! 

### Development Principles
- **Code Quality**: Comprehensive tests and clean, readable code
- **Performance First**: Every feature should be benchmarked
- **Learning Focus**: Document design decisions and trade-offs
- **Production Ready**: Build as if it's going to production

### Getting Started
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Add tests for your changes
4. Ensure benchmarks pass
5. Submit a pull request

---

## üìö Learning Resources

### Distributed Systems Concepts
- [Consistent Hashing Paper](https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/chash.pdf)
- [Gossip Protocols Overview](https://en.wikipedia.org/wiki/Gossip_protocol)  
- [CAP Theorem Explained](https://en.wikipedia.org/wiki/CAP_theorem)

### Go Performance
- [Go Memory Model](https://golang.org/ref/mem)
- [Effective Go](https://golang.org/doc/effective_go)
- [Go Concurrency Patterns](https://blog.golang.org/concurrency-patterns)

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Redis Team** for inspiration and architecture patterns
- **DragonFly Team** for the modern cache backend
- **Go Community** for excellent tooling and libraries
- **Distributed Systems Research** for foundational algorithms

---


**Built with ‚ù§Ô∏è for learning distributed systems**

[Report Bug](https://github.com/yourusername/distributed-cache/issues) ‚Ä¢ [Request Feature](https://github.com/yourusername/distributed-cache/issues) ‚Ä¢ [Documentation](https://github.com/yourusername/distributed-cache/wiki)

